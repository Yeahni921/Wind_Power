{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearnex import patch_sklearn, config_context\n",
    "patch_sklearn()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ipynb_path = os.getcwd()\n",
    "src_path = os.path.join(ipynb_path, 'src/')\n",
    "input_path = os.path.join(ipynb_path,\"input/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import scipy.stats as spst\n",
    "\n",
    "sys.path.append(src_path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from windpowerlib.wind_speed import logarithmic_profile\n",
    "from src.utils import uv_to_wsd # 윈도우에서는 앞에 src를 뺄것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power:  (155528, 29)\n",
      "train_y:  (52608, 4)\n",
      "LDAPS:  (235818, 15)\n"
     ]
    }
   ],
   "source": [
    "power_2020 = pd.read_parquet(input_path + \"dynamic_report_ewp02_2020_10min.parquet\").rename({'Date/Time': 'dt', 'WTG.Name': 'turbine_id'}, axis=1)[:-3]\n",
    "power_2021 = pd.read_parquet(input_path + \"dynamic_report_ewp02_2021_10min.parquet\").rename({'Date/Time': 'dt', 'WTG.Name': 'turbine_id'}, axis=1)[:-3]\n",
    "power_2022 = pd.read_parquet(input_path + \"dynamic_report_ewp02_2022_10min.parquet\").rename({'Date/Time': 'dt', 'WTG.Name': 'turbine_id'}, axis=1)[:-3]\n",
    "power = pd.concat([power_2020, power_2021, power_2022], ignore_index=True)\n",
    "\n",
    "gj_y = pd.read_parquet(input_path + \"train_y.parquet\").rename({'end_datetime': 'dt'}, axis=1)\n",
    "ldaps = pd.read_parquet(input_path + \"train_ldaps_gyeongju.parquet\")\n",
    "\n",
    "print(\"Power: \", power.shape)\n",
    "print(\"train_y: \", gj_y.shape)\n",
    "print(\"LDAPS: \", ldaps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /home/user/anaconda3/lib/python3.9/site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /home/user/anaconda3/lib/python3.9/site-packages (from seaborn) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.25 in /home/user/anaconda3/lib/python3.9/site-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /home/user/anaconda3/lib/python3.9/site-packages (from seaborn) (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/user/anaconda3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/user/anaconda3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/user/anaconda3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/user/anaconda3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/anaconda3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/user/anaconda3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/user/anaconda3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/user/anaconda3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/user/anaconda3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (6.1.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/user/anaconda3/lib/python3.9/site-packages (from pandas>=0.25->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/user/anaconda3/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.1->seaborn) (3.19.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/user/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Requirement already satisfied: astral in /home/user/anaconda3/lib/python3.9/site-packages (3.2)\n",
      "Collecting swifter\n",
      "  Using cached swifter-1.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: pandas>=1.0.0 in /home/user/anaconda3/lib/python3.9/site-packages (from swifter) (1.3.5)\n",
      "Requirement already satisfied: psutil>=5.6.6 in /home/user/anaconda3/lib/python3.9/site-packages (from swifter) (5.9.0)\n",
      "Requirement already satisfied: dask>=2.10.0 in /home/user/anaconda3/lib/python3.9/site-packages (from dask[dataframe]>=2.10.0->swifter) (2023.11.0)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in /home/user/anaconda3/lib/python3.9/site-packages (from swifter) (4.66.4)\n",
      "Requirement already satisfied: click>=8.1 in /home/user/anaconda3/lib/python3.9/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /home/user/anaconda3/lib/python3.9/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /home/user/anaconda3/lib/python3.9/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/anaconda3/lib/python3.9/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (24.1)\n",
      "Requirement already satisfied: partd>=1.2.0 in /home/user/anaconda3/lib/python3.9/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/user/anaconda3/lib/python3.9/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /home/user/anaconda3/lib/python3.9/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /home/user/anaconda3/lib/python3.9/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/user/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/user/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.0->swifter) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/user/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.0->swifter) (1.23.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/user/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.13.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.19.2)\n",
      "Requirement already satisfied: locket in /home/user/anaconda3/lib/python3.9/site-packages (from partd>=1.2.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/user/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas>=1.0.0->swifter) (1.16.0)\n",
      "Installing collected packages: swifter\n",
      "Successfully installed swifter-1.4.0\n",
      "Requirement already satisfied: scikit-learn-intelex in /home/user/anaconda3/lib/python3.9/site-packages (20230426.111118)\n",
      "Requirement already satisfied: daal4py>=2021.2 in /home/user/anaconda3/lib/python3.9/site-packages (from scikit-learn-intelex) (2023.1.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /home/user/anaconda3/lib/python3.9/site-packages (from scikit-learn-intelex) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/user/anaconda3/lib/python3.9/site-packages (from daal4py>=2021.2->scikit-learn-intelex) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/user/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.24->scikit-learn-intelex) (1.8.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/user/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.24->scikit-learn-intelex) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/user/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.24->scikit-learn-intelex) (2.2.0)\n",
      "Requirement already satisfied: windpowerlib in /home/user/anaconda3/lib/python3.9/site-packages (0.2.2)\n",
      "Requirement already satisfied: pandas in /home/user/anaconda3/lib/python3.9/site-packages (from windpowerlib) (1.3.5)\n",
      "Requirement already satisfied: requests in /home/user/anaconda3/lib/python3.9/site-packages (from windpowerlib) (2.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/user/anaconda3/lib/python3.9/site-packages (from pandas->windpowerlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/user/anaconda3/lib/python3.9/site-packages (from pandas->windpowerlib) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/user/anaconda3/lib/python3.9/site-packages (from pandas->windpowerlib) (1.23.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/anaconda3/lib/python3.9/site-packages (from requests->windpowerlib) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/anaconda3/lib/python3.9/site-packages (from requests->windpowerlib) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/anaconda3/lib/python3.9/site-packages (from requests->windpowerlib) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/anaconda3/lib/python3.9/site-packages (from requests->windpowerlib) (2024.7.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/user/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->windpowerlib) (1.16.0)\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.5.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/user/anaconda3/lib/python3.9/site-packages (from lightgbm) (1.23.5)\n",
      "Requirement already satisfied: scipy in /home/user/anaconda3/lib/python3.9/site-packages (from lightgbm) (1.8.1)\n",
      "Downloading lightgbm-4.5.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.5.0\n",
      "Collecting scikit-learn-extra\n",
      "  Downloading scikit_learn_extra-0.3.0-cp39-cp39-manylinux2010_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/user/anaconda3/lib/python3.9/site-packages (from scikit-learn-extra) (1.23.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/user/anaconda3/lib/python3.9/site-packages (from scikit-learn-extra) (1.8.1)\n",
      "Requirement already satisfied: scikit-learn>=0.23.0 in /home/user/anaconda3/lib/python3.9/site-packages (from scikit-learn-extra) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/user/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/user/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (2.2.0)\n",
      "Downloading scikit_learn_extra-0.3.0-cp39-cp39-manylinux2010_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-learn-extra\n",
      "Successfully installed scikit-learn-extra-0.3.0\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.1-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /home/user/anaconda3/lib/python3.9/site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/user/anaconda3/lib/python3.9/site-packages (from xgboost) (2.20.5)\n",
      "Requirement already satisfied: scipy in /home/user/anaconda3/lib/python3.9/site-packages (from xgboost) (1.8.1)\n",
      "Downloading xgboost-2.1.1-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.1\n"
     ]
    }
   ],
   "source": [
    "! pip install seaborn\n",
    "! pip install astral\n",
    "! pip install swifter\n",
    "! pip install scikit-learn-intelex\n",
    "!pip install windpowerlib\n",
    "! pip install lightgbm\n",
    "!pip install scikit-learn-extra\n",
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# yongmin's functions\n",
    "from src.utils import DataConnector\n",
    "from src.metric import NMAE\n",
    "from src.data_processor import *\n",
    "\n",
    "# model import\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235818, 23)\n"
     ]
    }
   ],
   "source": [
    "# 파이프라인 구성 및 적용\n",
    "DataPipeline = Pipeline([\n",
    "    ('uv_transform', UVTransformer('wind_u_10m', 'wind_v_10m')),\n",
    "    ('wind_transform', WindTransformer('wind_speed', 10, 100, ldaps['surf_rough'].mean())),\n",
    "    ('feature_engineering', FeatureTransformer()),\n",
    "])\n",
    "\n",
    "# 파이프라인을 이용하여 ldaps 데이터 변환\n",
    "ldaps_transformed = DataPipeline.fit_transform(ldaps)\n",
    "\n",
    "print(ldaps_transformed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_ldaps = ldaps_transformed.drop('turbine_id', axis=1).groupby('dt').mean()\n",
    "average_ldaps.columns = average_ldaps.columns.str.replace(r'[<>\\[\\]]', '_', regex=True)\n",
    "average_ldaps.columns = average_ldaps.columns.str.replace(r'[^\\w]', '_', regex=True)\n",
    "average_ldaps.columns = average_ldaps.columns.str.replace(r'__+', '_', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_ldaps.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_ldaps['dt'] = pd.to_datetime(average_ldaps['dt']).dt.tz_localize(None)\n",
    "gj_y['dt'] = pd.to_datetime(gj_y['dt']).dt.tz_localize(None)\n",
    "avg_data = pd.merge(average_ldaps, gj_y, on='dt', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_data_sorted = avg_data.sort_values(['dt', 'plant_name', 'energy_kwh'], ascending=[True, True, False])\n",
    "avg_data_cleaned = avg_data_sorted.drop_duplicates(subset=['dt', 'plant_name'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_data_cleaned = avg_data.drop_duplicates(subset=['dt'], keep='first')\n",
    "avg_data = avg_data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "def get_trasforms_datas(merged_data, numeric_columns, target):\n",
    "    z_scaler = StandardScaler()\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    \n",
    "    x_train = merged_data.loc[merged_data['dt'].between('2020-01-01', '2020-12-31', inclusive='left'), numeric_columns]\n",
    "    x_test = merged_data.loc[merged_data['dt'].between('2021-01-01', '2022-12-31', inclusive='left'), numeric_columns]\n",
    "\n",
    "    y_train = merged_data.loc[merged_data['dt'].between('2020-01-01', '2020-12-31', inclusive='left'), target].shift(periods = -24)\n",
    "    y_test = merged_data.loc[merged_data['dt'].between('2021-01-01', '2022-12-31', inclusive='left'), target].shift(periods = -24)\n",
    "    #y_train = y_train.dropna()\n",
    "    #y_test = y_test.dropna()\n",
    "\n",
    "    x_train = x_train.iloc[:-24]\n",
    "    y_train = y_train.iloc[:-24]\n",
    "\n",
    "    x_test = x_test.iloc[:-24]\n",
    "    y_test = y_test.iloc[:-24]\n",
    "\n",
    "    # Min-Max Scaling\n",
    "    x_train_m = minmax_scaler.fit_transform(x_train)\n",
    "    x_train_m = pd.DataFrame(x_train_m, columns=x_train.columns)\n",
    "    x_test_m = minmax_scaler.transform(x_test)\n",
    "    x_test_m = pd.DataFrame(x_test_m, columns=x_train.columns)\n",
    "\n",
    "    # Standard Scaling\n",
    "    x_train_z = z_scaler.fit_transform(x_train)\n",
    "    x_train_z = pd.DataFrame(x_train_z, columns=x_train.columns)\n",
    "    x_test_z = z_scaler.transform(x_test)\n",
    "    x_test_z = pd.DataFrame(x_test_z, columns=x_train.columns)\n",
    "\n",
    "    return x_train, x_test, x_train_m, x_test_m, x_train_z, x_test_z, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 이제 특징 생성에 클러스터링 결과는 보지 않을 예정.\n",
    "def addKmeansFeature(train_data, test_data):\n",
    "    pd.options.mode.chained_assignment = None\n",
    "\n",
    "    for n_clusters in range(2, 7):  # 2부터 6까지 클러스터 생성\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init=10)\n",
    "\n",
    "        train_data[f'cluster_{n_clusters}'] = kmeans.fit_predict(train_data[['wind_speed', 'wind_direction']])\n",
    "        \n",
    "        test_data[f'cluster_{n_clusters}'] = kmeans.predict(test_data[['wind_speed', 'wind_direction']])\n",
    "\n",
    "    return train_data, test_data\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def addPCAFeature(train_data, test_data):\n",
    "    # PCA 적용할 특징 열 선택 (u, v 성분)\n",
    "    wind_features = ['storm_u_5m', 'storm_v_5m', 'wind_u_10m', 'wind_v_10m', \n",
    "                     'wind_speed', 'wind_direction']\n",
    "    \n",
    "    # 훈련 데이터에서 PCA 학습\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_train = pca.fit_transform(train_data[wind_features])\n",
    "    \n",
    "    # 훈련 데이터에 주성분 추가\n",
    "    train_data['PC1'] = pca_train[:, 0]\n",
    "    train_data['PC2'] = pca_train[:, 1]\n",
    "    \n",
    "    # 테스트 데이터에 PCA 적용\n",
    "    pca_test = pca.transform(test_data[wind_features])\n",
    "    test_data['PC1'] = pca_test[:, 0]\n",
    "    test_data['PC2'] = pca_test[:, 1]\n",
    "\n",
    "    # PCA 설명력 확인\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    print(f\"PC1 설명력: {explained_variance[0]}\")\n",
    "    print(f\"PC2 설명력: {explained_variance[1]}\")\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "def addKMedoidsFeature(train_data, test_data):\n",
    "    pd.options.mode.chained_assignment = None\n",
    "\n",
    "    for n_clusters in range(2, 7):  # 2부터 6까지 클러스터 생성\n",
    "        kmedoids = KMedoids(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "        # 훈련 데이터에 K-Medoids 클러스터링 적용\n",
    "        train_data[f'medoid_cluster_{n_clusters}'] = kmedoids.fit_predict(train_data[['wind_speed', 'wind_direction']])\n",
    "\n",
    "        # 테스트 데이터에 학습된 K-Medoids 모델 적용\n",
    "        test_data[f'medoid_cluster_{n_clusters}'] = kmedoids.predict(test_data[['wind_speed', 'wind_direction']])\n",
    "\n",
    "    return train_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = avg_data.select_dtypes(include=['number']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, x_train_m, x_test_m, x_train_z, x_test_z, y_train, y_test = get_trasforms_datas(avg_data, numeric_columns, 'energy_kwh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmean 적용 완료\n",
      "PC1 설명력: 0.9982813596725464\n",
      "PC2 설명력: 0.0008244868949986994\n",
      "PC1 설명력: 0.7519216438993137\n",
      "PC2 설명력: 0.11667728909910816\n",
      "PC1 설명력: 0.333549415269656\n",
      "PC2 설명력: 0.32870070013054326\n",
      "pca 적용 완료\n",
      "kmedoid 적용 완료\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test = addKmeansFeature(x_train, x_test)\n",
    "x_train_m, x_test_m = addKmeansFeature(x_train_m, x_test_m)\n",
    "x_train_z, x_test_z = addKmeansFeature(x_train_z, x_test_z)\n",
    "print('kmean 적용 완료')\n",
    "x_train, x_test = addPCAFeature(x_train, x_test)\n",
    "x_train_m, x_test_m = addPCAFeature(x_train_m, x_test_m)\n",
    "x_train_z, x_test_z = addPCAFeature(x_train_z, x_test_z)\n",
    "print('pca 적용 완료')\n",
    "\n",
    "x_train, x_test = addKMedoidsFeature(x_train, x_test)\n",
    "x_train_m, x_test_m = addKMedoidsFeature(x_train_m, x_test_m)\n",
    "x_train_z, x_test_z = addKMedoidsFeature(x_train_z, x_test_z)\n",
    "print('kmedoid 적용 완료')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dict = {\n",
    "    # 원본 데이터만 사용\n",
    "    'original': ['elevation', 'land_cover', 'surf_rough', 'frictional_vmax_50m', 'frictional_vmin_50m', \n",
    "                 'pressure', 'relative_humid', 'specific_humid', 'temp_air', 'storm_u_5m', 'storm_v_5m', \n",
    "                 'wind_u_10m', 'wind_v_10m', 'wind_speed', 'wind_direction', 'wind_speed_100m'],\n",
    "\n",
    "    # PCA 추가\n",
    "    'pca_only': ['elevation', 'land_cover', 'surf_rough', 'frictional_vmax_50m', 'frictional_vmin_50m', \n",
    "                 'pressure', 'relative_humid', 'specific_humid', 'temp_air', 'storm_u_5m', 'storm_v_5m', \n",
    "                 'wind_u_10m', 'wind_v_10m', 'wind_speed', 'wind_direction', 'wind_speed_100m', 'PC1', 'PC2'],\n",
    "\n",
    "    # 클러스터(2~6) 추가\n",
    "    'cluster_only': ['elevation', 'land_cover', 'surf_rough', 'frictional_vmax_50m', 'frictional_vmin_50m', \n",
    "                     'pressure', 'relative_humid', 'specific_humid', 'temp_air', 'storm_u_5m', 'storm_v_5m', \n",
    "                     'wind_u_10m', 'wind_v_10m', 'wind_speed', 'wind_direction', 'wind_speed_100m', \n",
    "                     'cluster_2', 'cluster_3', 'cluster_4', 'cluster_5', 'cluster_6'],\n",
    "\n",
    "    # PCA + 클러스터(2~6) 추가\n",
    "    'pca_and_cluster': ['elevation', 'land_cover', 'surf_rough', 'frictional_vmax_50m', 'frictional_vmin_50m', \n",
    "                        'pressure', 'relative_humid', 'specific_humid', 'temp_air', 'storm_u_5m', 'storm_v_5m', \n",
    "                        'wind_u_10m', 'wind_v_10m', 'wind_speed', 'wind_direction', 'wind_speed_100m', \n",
    "                        'PC1', 'PC2', 'cluster_2', 'cluster_3', 'cluster_4', 'cluster_5', 'cluster_6'],\n",
    "\n",
    "    # 클러스터 개수에 따른 경우\n",
    "    'cluster_2_only': ['elevation', 'land_cover', 'surf_rough', 'frictional_vmax_50m', 'frictional_vmin_50m', \n",
    "                       'pressure', 'relative_humid', 'specific_humid', 'temp_air', 'storm_u_5m', 'storm_v_5m', \n",
    "                       'wind_u_10m', 'wind_v_10m', 'wind_speed', 'wind_direction', 'wind_speed_100m', 'cluster_2'],\n",
    "    \n",
    "    'cluster_3_only': ['elevation', 'land_cover', 'surf_rough', 'frictional_vmax_50m', 'frictional_vmin_50m', \n",
    "                       'pressure', 'relative_humid', 'specific_humid', 'temp_air', 'storm_u_5m', 'storm_v_5m', \n",
    "                       'wind_u_10m', 'wind_v_10m', 'wind_speed', 'wind_direction', 'wind_speed_100m', 'cluster_3'],\n",
    "    \n",
    "    'cluster_4_only': ['elevation', 'land_cover', 'surf_rough', 'frictional_vmax_50m', 'frictional_vmin_50m', \n",
    "                       'pressure', 'relative_humid', 'specific_humid', 'temp_air', 'storm_u_5m', 'storm_v_5m', \n",
    "                       'wind_u_10m', 'wind_v_10m', 'wind_speed', 'wind_direction', 'wind_speed_100m', 'cluster_4'],\n",
    "    \n",
    "    'cluster_5_only': ['elevation', 'land_cover', 'surf_rough', 'frictional_vmax_50m', 'frictional_vmin_50m', \n",
    "                       'pressure', 'relative_humid', 'specific_humid', 'temp_air', 'storm_u_5m', 'storm_v_5m', \n",
    "                       'wind_u_10m', 'wind_v_10m', 'wind_speed', 'wind_direction', 'wind_speed_100m', 'cluster_5'],\n",
    "    \n",
    "    'cluster_6_only': ['elevation', 'land_cover', 'surf_rough', 'frictional_vmax_50m', 'frictional_vmin_50m', \n",
    "                       'pressure', 'relative_humid', 'specific_humid', 'temp_air', 'storm_u_5m', 'storm_v_5m', \n",
    "                       'wind_u_10m', 'wind_v_10m', 'wind_speed', 'wind_direction', 'wind_speed_100m', 'cluster_6'],\n",
    "\n",
    "    # KMedoids 추가\n",
    "    'kmedoids_only': ['elevation', 'land_cover', 'surf_rough', 'frictional_vmax_50m', 'frictional_vmin_50m', \n",
    "                      'pressure', 'relative_humid', 'specific_humid', 'temp_air', 'storm_u_5m', 'storm_v_5m', \n",
    "                      'wind_u_10m', 'wind_v_10m', 'wind_speed', 'wind_direction', 'wind_speed_100m', \n",
    "                      'medoid_cluster_2', 'medoid_cluster_3', 'medoid_cluster_4', 'medoid_cluster_5', 'medoid_cluster_6'],\n",
    "\n",
    "    # PCA + KMedoids 추가\n",
    "    'pca_and_kmedoids': ['elevation', 'land_cover', 'surf_rough', 'frictional_vmax_50m', 'frictional_vmin_50m', \n",
    "                         'pressure', 'relative_humid', 'specific_humid', 'temp_air', 'storm_u_5m', 'storm_v_5m', \n",
    "                         'wind_u_10m', 'wind_v_10m', 'wind_speed', 'wind_direction', 'wind_speed_100m', \n",
    "                         'PC1', 'PC2', 'medoid_cluster_2', 'medoid_cluster_3', 'medoid_cluster_4', 'medoid_cluster_5', 'medoid_cluster_6'],\n",
    "\n",
    "    # KMedoids 클러스터 개수에 따른 경우\n",
    "    'medoid_cluster_2_only': ['elevation', 'land_cover', 'surf_rough', 'frictional_vmax_50m', 'frictional_vmin_50m', \n",
    "                              'pressure', 'relative_humid', 'specific_humid', 'temp_air', 'storm_u_5m', 'storm_v_5m', \n",
    "                              'wind_u_10m', 'wind_v_10m', 'wind_speed', 'wind_direction', 'wind_speed_100m', 'medoid_cluster_2'],\n",
    "\n",
    "    'medoid_cluster_3_only': ['elevation', 'land_cover', 'surf_rough', 'frictional_vmax_50m', 'frictional_vmin_50m', \n",
    "                              'pressure', 'relative_humid', 'specific_humid', 'temp_air', 'storm_u_5m', 'storm_v_5m', \n",
    "                              'wind_u_10m', 'wind_v_10m', 'wind_speed', 'wind_direction', 'wind_speed_100m', 'medoid_cluster_3'],\n",
    "\n",
    "    'medoid_cluster_4_only': ['elevation', 'land_cover', 'surf_rough', 'frictional_vmax_50m', 'frictional_vmin_50m', \n",
    "                              'pressure', 'relative_humid', 'specific_humid', 'temp_air', 'storm_u_5m', 'storm_v_5m', \n",
    "                              'wind_u_10m', 'wind_v_10m', 'wind_speed', 'wind_direction', 'wind_speed_100m', 'medoid_cluster_4'],\n",
    "\n",
    "    'medoid_cluster_5_only': ['elevation', 'land_cover', 'surf_rough', 'frictional_vmax_50m', 'frictional_vmin_50m', \n",
    "                              'pressure', 'relative_humid', 'specific_humid', 'temp_air', 'storm_u_5m', 'storm_v_5m', \n",
    "                              'wind_u_10m', 'wind_v_10m', 'wind_speed', 'wind_direction', 'wind_speed_100m', 'medoid_cluster_5'],\n",
    "\n",
    "    'medoid_cluster_6_only': ['elevation', 'land_cover', 'surf_rough', 'frictional_vmax_50m', 'frictional_vmin_50m', \n",
    "                              'pressure', 'relative_humid', 'specific_humid', 'temp_air', 'storm_u_5m', 'storm_v_5m', \n",
    "                              'wind_u_10m', 'wind_v_10m', 'wind_speed', 'wind_direction', 'wind_speed_100m', 'medoid_cluster_6']\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.deepTrain_roughVer.Analysis_WindTurbine.Model.RNNs import RNN, LSTM, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "def NMAE(y_true, y_pred):\n",
    "    \"\"\"Normalized Mean Absolute Error (NMAE) 계산 함수.\"\"\"\n",
    "    return mean_absolute_error(y_true, y_pred) / (sum(abs(y_true)) / len(y_true)) * 100\n",
    "\n",
    "def train_rnn_model(save_dir, model, x_train, y_train, x_test, y_test, epochs=200, batch_size=30, lr=0.001):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    x_train_tensor = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "    x_test_tensor = torch.tensor(x_test.values, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        y_pred_list = []\n",
    "        y_true_list = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "                y_pred_list.extend(outputs.cpu().numpy())\n",
    "                y_true_list.extend(targets.cpu().numpy())\n",
    "\n",
    "        y_pred_list = np.array(y_pred_list).flatten()\n",
    "        y_true_list = np.array(y_true_list).flatten()\n",
    "\n",
    "        mae = mean_absolute_error(y_true_list, y_pred_list)\n",
    "        nmae = NMAE(y_true_list, y_pred_list)\n",
    "        r2 = r2_score(y_true_list, y_pred_list)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, f\"{model.__class__.__name__}_best_model.pth\"))\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "        print(f\"Val Loss: {val_loss/len(test_loader):.4f}, MAE: {mae:.4f}, NMAE: {nmae:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f\"{model.__class__.__name__}_final_model.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [17370, 2223360]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m rnn_model \u001b[38;5;241m=\u001b[39m RNNModule(input_dim\u001b[38;5;241m=\u001b[39mx_train_m_selected\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m     10\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ipynb_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnotebooks/test_avg/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_RNN_m/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrain_rnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train_m_selected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test_m_selected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# LSTM 모델 학습\u001b[39;00m\n\u001b[1;32m     14\u001b[0m lstm_model \u001b[38;5;241m=\u001b[39m LSTMModule(input_dim\u001b[38;5;241m=\u001b[39mx_train_m_selected\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n",
      "Cell \u001b[0;32mIn[34], line 53\u001b[0m, in \u001b[0;36mtrain_rnn_model\u001b[0;34m(save_dir, model, x_train, y_train, x_test, y_test, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m     50\u001b[0m y_pred_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_pred_list)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     51\u001b[0m y_true_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_true_list)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m---> 53\u001b[0m mae \u001b[38;5;241m=\u001b[39m \u001b[43mmean_absolute_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m nmae \u001b[38;5;241m=\u001b[39m NMAE(y_true_list, y_pred_list)\n\u001b[1;32m     55\u001b[0m r2 \u001b[38;5;241m=\u001b[39m r2_score(y_true_list, y_pred_list)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:204\u001b[0m, in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    141\u001b[0m     {\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m ):\n\u001b[1;32m    152\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_absolute_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    0.85...\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    208\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(np\u001b[38;5;241m.\u001b[39mabs(y_pred \u001b[38;5;241m-\u001b[39m y_true), weights\u001b[38;5;241m=\u001b[39msample_weight, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:99\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m        correct keyword.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    101\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    407\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    412\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [17370, 2223360]"
     ]
    }
   ],
   "source": [
    "for key in x_dict.keys():\n",
    "    # ====================\n",
    "    # Min-Max 정규화 데이터\n",
    "    # ====================\n",
    "    x_train_m_selected = x_train_m[x_dict[key]]\n",
    "    x_test_m_selected = x_test_m[x_dict[key]]\n",
    "\n",
    "    # RNN 모델 학습\n",
    "    rnn_model = RNN(input_dim=x_train_m_selected.shape[1], hidden_dim=128)\n",
    "    save_dir = os.path.join(ipynb_path, f'notebooks/test_avg/{key}_RNN_m/')\n",
    "    train_rnn_model(save_dir, rnn_model, x_train_m_selected, y_train, x_test_m_selected, y_test)\n",
    "\n",
    "    # LSTM 모델 학습\n",
    "    lstm_model = LSTMModule(input_dim=x_train_m_selected.shape[1], hidden_dim=128)\n",
    "    save_dir = os.path.join(ipynb_path, f'notebooks/test_avg/{key}_LSTM_m/')\n",
    "    train_rnn_model(save_dir, lstm_model, x_train_m_selected, y_train, x_test_m_selected, y_test)\n",
    "\n",
    "    # GRU 모델 학습\n",
    "    gru_model = GRUModule(input_dim=x_train_m_selected.shape[1], hidden_dim=128)\n",
    "    save_dir = os.path.join(ipynb_path, f'notebooks/test_avg/{key}_GRU_m/')\n",
    "    train_rnn_model(save_dir, gru_model, x_train_m_selected, y_train, x_test_m_selected, y_test)\n",
    "\n",
    "    # ====================\n",
    "    # z-정규화 데이터\n",
    "    # ====================\n",
    "    x_train_z_selected = x_train_z[x_dict[key]]\n",
    "    x_test_z_selected = x_test_z[x_dict[key]]\n",
    "\n",
    "    # RNN 모델 학습\n",
    "    rnn_model = RNNModule(input_dim=x_train_z_selected.shape[1], hidden_dim=64)\n",
    "    save_dir = os.path.join(ipynb_path, f'notebooks/test_avg/{key}_RNN_z/')\n",
    "    train_rnn_model(save_dir, rnn_model, x_train_z_selected, y_train, x_test_z_selected, y_test)\n",
    "\n",
    "    # LSTM 모델 학습\n",
    "    lstm_model = LSTMModule(input_dim=x_train_z_selected.shape[1], hidden_dim=64)\n",
    "    save_dir = os.path.join(ipynb_path, f'notebooks/test_avg/{key}_LSTM_z/')\n",
    "    train_rnn_model(save_dir, lstm_model, x_train_z_selected, y_train, x_test_z_selected, y_test)\n",
    "\n",
    "    # GRU 모델 학습\n",
    "    gru_model = GRUModule(input_dim=x_train_z_selected.shape[1], hidden_dim=64)\n",
    "    save_dir = os.path.join(ipynb_path, f'notebooks/test_avg/{key}_GRU_z/')\n",
    "    train_rnn_model(save_dir, gru_model, x_train_z_selected, y_train, x_test_z_selected, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
